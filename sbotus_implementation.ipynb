{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sbotus implementation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timschott/kratos/blob/master/sbotus_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of GPT-2 on Supreme Court sentences.\n",
        "\n",
        "by [Tim Schott](https://timschott.com), leaning heavily on the tutorial by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: August 2020*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read this [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a4b364a6-7153-47d6-aee1-2861cb9d042d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Aug  4 03:21:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below. So we're going to use the default model particularly because the dataset I'm working with is not enormous\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4718880b-6632-4412-a2a7-cffb63bcdcb8"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 240Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 118Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 780Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 200Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 269Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 175Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 150Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af3f793e-a6cd-4a4f-c4e4-2232ee39f24c"
      },
      "source": [
        "## gpt2.mount_gdrive()\n",
        "!google-drive-ocamlfuse -cc"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: google-drive-ocamlfuse: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Of course, for my purposes, I will need to contiually swap through this set up, since I have multiple training files. I can experiment with an approach to this once I get one Justice working as a proof of concept. This file mounting thing is pretty buggy though so mileage may vary.\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89e9e690-4dbd-417d-94a0-16988cf6d43b"
      },
      "source": [
        "file_name = \"KENNEDYsents.txt\"\n",
        "print(file_name)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KENNEDYsents.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If a text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM. -- Skipping this for my individual justice files, which top out at 3 mb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2 - Creating the Model that will generate your text.\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ba1307b-15ab-4990-c01e-a7ad5a7d8615"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='kennedy1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 208332 tokens\n",
            "Training...\n",
            "[10 | 28.61] loss=3.50 avg=3.50\n",
            "[20 | 50.62] loss=3.30 avg=3.40\n",
            "[30 | 73.13] loss=3.52 avg=3.44\n",
            "[40 | 96.44] loss=3.25 avg=3.39\n",
            "[50 | 120.95] loss=3.35 avg=3.38\n",
            "[60 | 145.19] loss=3.07 avg=3.33\n",
            "[70 | 168.88] loss=3.33 avg=3.33\n",
            "[80 | 192.75] loss=2.85 avg=3.27\n",
            "[90 | 216.90] loss=2.78 avg=3.21\n",
            "[100 | 240.87] loss=3.02 avg=3.19\n",
            "[110 | 264.71] loss=2.92 avg=3.17\n",
            "[120 | 288.60] loss=2.78 avg=3.13\n",
            "[130 | 312.53] loss=2.77 avg=3.10\n",
            "[140 | 336.49] loss=2.63 avg=3.07\n",
            "[150 | 360.52] loss=2.67 avg=3.04\n",
            "[160 | 384.59] loss=2.68 avg=3.01\n",
            "[170 | 408.61] loss=2.39 avg=2.97\n",
            "[180 | 432.56] loss=2.20 avg=2.93\n",
            "[190 | 456.48] loss=2.31 avg=2.89\n",
            "[200 | 480.38] loss=2.20 avg=2.85\n",
            "======== SAMPLE 1 ========\n",
            " a significant disadvantage that would likely lead to more abortions\n",
            "in doing so, it would counter the advances made by and of the right to know\n",
            "see\n",
            "\" state\n",
            "in its own words and in its actions and its findings:\n",
            "\" pro\n",
            "\" and in a case brought under and in which we are now confronted\n",
            "the term \"counsel position\" refers to the position of the client in the case\n",
            "it is alleged here that the \"public affairs\" director of had knowledge that an unborn child was being treated, born, or was in the hands of \"a group of persons\" organized to perverted ends\n",
            "\" position, and so forth, relate to the types of positions and communication\n",
            "ante is not a law against \"advocacy\" or \"propaganda\" or for that matter, for that matter, for\n",
            "we are told in a state where \"law suits\" to counter against \"propaganda\" were once the norm, that no longer is\n",
            "this is not a defense which seeks to impose upon the state by compulsion or trickery a protective or sanction\n",
            "that is not a law against misleading respondents\n",
            "and in this case, a clear assertion of the plaintiffs interests is sufficient\n",
            "the court finds that respondents cannot prevail, and that its exercise was narrowly drawn\n",
            "it does, however, support plaintiffs contention, as do we, that respondents interest in maintaining the right of protected speech does not\n",
            "nor does respondents challenge the state laws in any substantial way\n",
            "as we shall explain below, we think proper to apply the same scrutiny to respondents particular interests, as we have to in a criminal context\n",
            "respondents claim that if the state law were to be enforced at the very time that it was passed, respondents may well have suffered serious emotional, physical, and financial damages\n",
            "they also say the law violates three fundamental first amendment rights\n",
            "they include respondents right to a knowing, informed, and accountable representative body for the information required to determine whether a candidate has expressed his views according to policy\n",
            "as we have stated, respondents right to an informed, informed, accountable body is not inviolable, as we found in the cases we have cited\n",
            "respondents also say the state is violating their first amendment rights by failing to maintain a record of candidates for both houses of the legislature\n",
            "they say the records are kept to perfunctory terms of the case and used in an attempt to \"intimidate\" voters\n",
            "and that the use of the records to \"intimidate\" voters was prohibited by in part\n",
            "they also say the state is violating the first amendment by passing laws that violate legitimate first amendment interests\n",
            "we agree with the courts decision in the instant cases, and agree with their interpretation of this case\n",
            "to sustain a statute that imposes serious limitations on a speech act under the first amendment, is simply a sham in many respects\n",
            "it imposes no legal requirement at all, and it imposes a significant burden on a person's choice of medium by which to communicate his thoughts or his views\n",
            "even if the act were intended to prevent, or deter, speech in another way, it is not intended to prevent speech in a substantive manner\n",
            "furthermore, the statute imposes a very real risk of injury to constitutional protection and that protected expression in general\n",
            "the danger lies in the statute proscribes the conduct that the defendant in so hoped would be considered \"neutral\" by the state\n",
            "respondents allege a substantial injury to the first amendment that the court finds substantial because the statute is not directed at the speech of the speaker, rather at the actions of the recipient of the communication, the state\n",
            "they also allege a substantial harm to the constitutional interest that the statute compels, namely, the right of free and unfettered expression aimed at those advocating or encouraging other speakers to speak\n",
            "the statute, they say, can serve as an \"intrusive tool\" for state censors and government censors\n",
            "they further allege a purposeful interference with the free, unfettered exercise of the first amendment rights guaranteed by the clause\n",
            "they add that their concerns are insufficient to justify a statute that punishes speech altogether\n",
            "we disagree\n",
            "we hold that this statute is so-called \"bias\" neutral that any speech speech judged \"likely to be of interest to the state is targeted in a manner not likely to be influenced by any speakers opinion\n",
            "we decline respondents request for a new trial, and remand before this court for further proceedings on the argument that respondent is not entitled to qualified immunity as to protected protected speech in this case\n",
            "we hold that the laws by their terms violate the clause because of their first amendment operation\n",
            "the majority advances two theories of why and how the act, as drafted, can be used to punish certain unprotected speech\n",
            "first, the enactments in question were enacted during the first days of a new president, and the act is not unusual for this court to adopt invalidation of state laws in a presidential election\n",
            "the second theory of attack comes not from our traditional cases, but from a constitutional tradition of restraint over the first amendment, in which courts are\n",
            "\n",
            "[210 | 515.92] loss=2.41 avg=2.83\n",
            "[220 | 539.93] loss=2.42 avg=2.81\n",
            "[230 | 563.97] loss=2.19 avg=2.78\n",
            "[240 | 588.02] loss=1.92 avg=2.74\n",
            "[250 | 612.02] loss=2.03 avg=2.71\n",
            "[260 | 635.92] loss=2.22 avg=2.69\n",
            "[270 | 659.83] loss=1.52 avg=2.64\n",
            "[280 | 683.69] loss=1.78 avg=2.60\n",
            "[290 | 707.57] loss=1.71 avg=2.57\n",
            "[300 | 731.49] loss=1.73 avg=2.53\n",
            "[310 | 755.42] loss=2.02 avg=2.52\n",
            "[320 | 779.34] loss=1.49 avg=2.48\n",
            "[330 | 803.30] loss=1.12 avg=2.43\n",
            "[340 | 827.27] loss=1.00 avg=2.38\n",
            "[350 | 851.24] loss=1.15 avg=2.34\n",
            "[360 | 875.22] loss=1.40 avg=2.31\n",
            "[370 | 899.15] loss=0.89 avg=2.26\n",
            "[380 | 923.05] loss=1.01 avg=2.22\n",
            "[390 | 946.96] loss=1.07 avg=2.19\n",
            "[400 | 970.89] loss=1.05 avg=2.15\n",
            "======== SAMPLE 1 ========\n",
            "ging on public holidays\n",
            "with respect to holidays other than those not commemorative of previous presidents, only certain civic holidays, such as the opening of new weeks and months and the opening of new months, are exempted\n",
            "that is, public holidays and holidays not traditionally marked by sacrifice, peace sign or burning of the flag are not criminal activities that can be supported under federal law by reason of their expressive activities\n",
            "religion alone, furthermore, does not bear the weighty claim of exemption under the clause if it is not the predominant \"expression of religious thought or belief\" inherent in or centered along the four creche days worship is permitted under this exemption\n",
            "the burden falls squarely on the tomar sponsor to sustain a literal interpretation of the government speech policy\n",
            "for this reason concur in the judgment of the court\n",
            "the of the ofare not created \"solely to serve the general public interest in recognition of the vital role religion plays in our society\n",
            "they are simply a recognition that, by fostering greater civic participation, these societies around the recognize and encourage their citizens to step forward and say the government is their Lord\n",
            "even churches, in contrast, are often \"closed\" or \"marginalized\" locations where members can worship simply by walking through the doors of a given parishioner or ward\n",
            "these circumstances, and others like them, foster an atmosphere of hostility, an atmosphere that can undermine the motivation of worship and hinder the voluntary adherence of a faithful to the worship they believe in\n",
            "these factors bear on the defense of the against a prosecution as to \"lawful purposes\" and \"lawful utterances\" in the definition of \"lawful words\" is considerable\n",
            "most of all, the significant difference in the defense counsel compensation in the military, paid in the vast majority of cases, and the statutory standard we use to derive these percentages reflects general differences in the legal profession\n",
            "in the military the prevailing class was either full time employees, part time or full time on the staff\n",
            "in the court above analyzed the governmental performance of the in parallel, with an emphasis added at the end for clarity\n",
            "the key finding of opinion is that for many service members and their dependents, see id\n",
            "even with regard to government payments, there is an intrinsic and material elasticity inherent in government speech and expression\n",
            "this, in turn, permits the recipient of government payments to convey his message effectively, without censoring his speech or expression for practical effect\n",
            "there is no guarantee that such messages will be successful or effective; but whatever the inevitable effects a particular form of government speech or expression will have, whatever the practical impossibility of persuasion faces most service members, their dependents and their caregivers, it is those citizens who most value the ability of all citizens to express ideas and propose new forms of expression that seek to reconcile the past with the present, see quoting\n",
            "there can be little doubt as to the utility of extending the statutory hours limit to service members and their dependents for six months a year is an \"essential component of the\n",
            "th the court below analyzed the standard used to derive the necessary rough estimates of statutory lengths for various speakers and their dependents, and we agree with this assessment\n",
            "hours are an important part of our cultural tradition\n",
            "they give meaning and depth to words and actions, both in writing and in spoken form\n",
            "the time spent devoted to expressive activities, however, are not merely incidental to the written messages that pass for written messages\n",
            "rather, the difference between these two spheres depends in large part on how much the listener and the speaker \"engage in a process of critical thinking\"that is, how fundamentally shortsighted our society must be about to lead\n",
            "this process of critical thinking, which we have described as the principal analytical method of the state, must in fact be the more essential and more essential for a meaningful free society if we are to achieve our professed aims as stated,\n",
            "we must reject as simply ahistirtaking our placeballoting\n",
            "and must take cognizance of the fact that our legislators often engage in critical thinking that leads to their candidacies\n",
            "this critical thinking must be further analyzed in further proceedings, but first we must ask how farforward this critical thinking must be if the court, by interpreting the time period right out of the blue, by requiring detailed license plate readers in every state and requiring registration in every non-territorial state, certifies to the licensing authority that the speaker is indeed within the time period permitted under the fourteenth amendments requirements?if the answer is yes, then so be it, for the result may not be as clear cut as one of the central judicial principles stated inthe court struck down a number of state laws aimed at burdening and restricting speech\n",
            "in this case the court invalidated an election commission report that showed certain long-standing voter restrictions existed a state could require a full license renewal for all voters who had applied for them, provided they do not obstruct the legitimate administration of justice by voters or their parties\n",
            "the case developed into a full state action under in part to ensure that application for voter-suppression permits did\n",
            "\n",
            "[410 | 1005.29] loss=0.98 avg=2.12\n",
            "[420 | 1029.27] loss=0.77 avg=2.08\n",
            "[430 | 1053.24] loss=0.84 avg=2.04\n",
            "[440 | 1077.23] loss=1.05 avg=2.02\n",
            "[450 | 1101.21] loss=0.63 avg=1.98\n",
            "[460 | 1125.21] loss=0.67 avg=1.94\n",
            "[470 | 1149.20] loss=0.65 avg=1.91\n",
            "[480 | 1173.19] loss=0.48 avg=1.87\n",
            "[490 | 1197.22] loss=0.62 avg=1.84\n",
            "[500 | 1221.27] loss=0.68 avg=1.81\n",
            "Saving checkpoint/kennedy1/model-500\n",
            "[510 | 1247.83] loss=0.49 avg=1.78\n",
            "[520 | 1271.94] loss=0.61 avg=1.75\n",
            "[530 | 1295.94] loss=0.46 avg=1.72\n",
            "[540 | 1319.83] loss=0.28 avg=1.68\n",
            "[550 | 1343.71] loss=0.29 avg=1.65\n",
            "[560 | 1367.68] loss=0.45 avg=1.62\n",
            "[570 | 1391.71] loss=0.41 avg=1.59\n",
            "[580 | 1415.85] loss=0.45 avg=1.57\n",
            "[590 | 1439.91] loss=0.34 avg=1.54\n",
            "[600 | 1463.93] loss=0.35 avg=1.51\n",
            "======== SAMPLE 1 ========\n",
            "con-section extension of time\n",
            "the court says the legislative power to extend time may be explained as a necessary evil for, say, to purge an entire literature warehouse from its books and disable certain members only if they are convicted of certain crimes\n",
            "why could the legislature cure this evil with an extension of the age restriction? statute has power to do so\n",
            "there is, however, no authority for the legislatures to prescribe the time needed for the granting and suspending of these exclusive powers\n",
            "i do not think we should, under our precedents, put so helplessly at odds with the essential rights we seek to protect\n",
            "time is money\n",
            "money is power\n",
            "time is scarce legislative gold\n",
            "again and the government show no sign of abating, either\n",
            "time is precious and valuable legislation has its defenders and its defenders also\n",
            "the court puts its first amendment doctrine above all else\n",
            "for the record, amicus does not represent the views of the government\n",
            "HEALTH CARE\n",
            "\"is designed to deter, punish, and remedialate the criminal acts that blight the lives and property of convicted criminals throughout our country\n",
            "the courts first amendment studies\n",
            "hearing on vii\n",
            "the court identified six categories of political speech: television ads, billboards, leaflets, and direct mail from outside\n",
            "in the broadcast stage alone, there were approximatelya speech per se that per se was likely to be restricted by the government\n",
            "speech on the licensed in-person stage alone was sufficient to stop or restrict criminal activity\n",
            "speech made of statutory material, is speech of grave national importance\n",
            "the majority recognized this and, relying on that rationale, invalidated the act\n",
            "speech on the non-profit stage alone would suffice to draw first amendment lines\n",
            "in its brief filed as amicus the court acknowledged the draw was \"generous in its application, its analysis drawing a fine distinction, between speech which protests against imprisonment and speech which protests against parole and sentencing\n",
            ": atconsciously or inflexible as to the definition of prison rule\n",
            "we struck down a mandatory minimum sentence for distribution of election literature, but the line was flexible in various respects\n",
            "it was established to respond to the significant deterrent effect campaigns have on the innocent-until-proven guilty voting public, to the distress of the appellant and his family, it made no distinction between speech which protests against imprisonment and speech which protests against parole and sentencing\n",
            "in the plurality held the statutory distinction difficult to distinguish from state restrictions on political speech, as well as from the unconstitutional targeting claim of the statute\n",
            "placing particular emphasis on the distinction between speech to support a criminal defendant in prison and speech to support his release from prison infers that both are speech that can support liability under the first amendment\n",
            "in addition makes explicit the principle that no statute addresses the danger that voters or their officials may be arrested or convicted for vote fraud or want to know more about that\n",
            "declaration of infolo, atthe court reaffirmed the prior ruling that was invalid paul and the court now held invalid statute after it was enacted, as was true with respect to prior attempts to ban soliciting prostitutes in and prior attempts to provide medical treatment for voters with persistent intolerance for medical patients who were not candidates\n",
            "as was also true of the reasoned with respect to the statutes profligacy and perversion mandate, the majority reaffirmed the prohibitions and concluded the was necessary to protect voters and their treatment of their representatives\n",
            "declaration of infolo, the court returned the same conclusion\n",
            "in re state of the art prosecutors and district judges offices, as well as voter registration applications\n",
            "in w elections\n",
            "we have no doubt that on first amendment grounds, if a speech statute restricts speech to protect candidates or their offices, there is a majority in the to ensure his or her speech is not disrupted\n",
            "in we held said interrupted did not raise a first amendment issue, and added that\n",
            "in do however, as the plurality noted,\n",
            "the statute prevents a licensed meat dispensary from dispensing its patients with false information\n",
            "this simple fact remains behind the haste and obvious perversion and extortion involved in the facilitating the crime of bribery\n",
            "publius now argues that consumers who choose to subscribe to thecensored comment on its site will be deprived of their right to know\n",
            "we decline your invitation to review this case\n",
            "satireallegedly, the candidate in question might well be a corrupt lobbyist with a business background\n",
            "perhaps he wishes to obtain government favors through bribery\n",
            "before engaging in personal misconduct, however, a private person may impermissibly distance himself from the efforts of the lobbying operation\n",
            "in this instance, the court suspended the statute, and the matter is moot\n",
            "for most of modern lobbying has involved receiving money from a private person\n",
            "since the firms ability to solicit and favor other firms and prevent them from receiving favorable information about lobbying\n",
            "in andyou have an interest in amending the statute so that bribery is no longer an issue?\n",
            "what the legislation would be?\n",
            "i have a difficult question to answer\n",
            "suppose that you prevail\n",
            "that you win?\n",
            "so that the public gets very interested in the\n",
            "\n",
            "[610 | 1498.33] loss=0.43 avg=1.49\n",
            "[620 | 1522.24] loss=0.22 avg=1.46\n",
            "[630 | 1546.15] loss=0.19 avg=1.44\n",
            "[640 | 1570.13] loss=0.17 avg=1.41\n",
            "[650 | 1594.13] loss=0.15 avg=1.38\n",
            "[660 | 1618.13] loss=0.18 avg=1.36\n",
            "[670 | 1642.19] loss=0.21 avg=1.34\n",
            "[680 | 1666.19] loss=0.17 avg=1.31\n",
            "[690 | 1690.22] loss=0.18 avg=1.29\n",
            "[700 | 1714.23] loss=0.20 avg=1.27\n",
            "[710 | 1738.26] loss=0.13 avg=1.25\n",
            "[720 | 1762.31] loss=0.17 avg=1.22\n",
            "[730 | 1786.35] loss=0.17 avg=1.20\n",
            "[740 | 1810.42] loss=0.19 avg=1.18\n",
            "[750 | 1834.44] loss=0.18 avg=1.17\n",
            "[760 | 1858.43] loss=0.15 avg=1.15\n",
            "[770 | 1882.44] loss=0.13 avg=1.13\n",
            "[780 | 1906.45] loss=0.11 avg=1.11\n",
            "[790 | 1930.46] loss=0.19 avg=1.09\n",
            "[800 | 1954.49] loss=0.16 avg=1.08\n",
            "======== SAMPLE 1 ========\n",
            " have to pay for the speech of the audience\n",
            "in some cases, if there are lots of people engaging in the same expression there must be an expressive purpose\n",
            "there must be a purpose behind it all\n",
            "why cant you just do it by printing the statement?\n",
            "does the city have a free press when it is printing the statement?\n",
            "i have a different point\n",
            "the constitution just doesnt depend on the printing in a forum\n",
            "the simply prints what the paper is presented with\n",
            "well, you mean i think the more abstract\n",
            "i think the more abstract\n",
            "he can say more abstract thoughts about subject matter altogether\n",
            "well, then print the whole thing in the newspaper\n",
            "i take it you dont believe in quoting\n",
            "the newspaper simply prints what the paper is presented with\n",
            "would you agree that a quote is a corporate form for phrases susceptible of recall by the newspaper?\n",
            "what is the constitutional holding that a newspaper cant prohibit quotations that are facially misleading?\n",
            "well, is there a principle, or an empirical observation that show that a newspaper can always recover?\n",
            "what is there in the record that show that a quote is false?\n",
            "but was not answering the question\n",
            "well, you keep telling me that if you cannot win this case, youre going to have to have the trial\n",
            "could you win the case?\n",
            "im not answering the question\n",
            "were you answering the question?\n",
            "are you answering the question?\n",
            "the whole underpinning of the case is this: you cannot permit an ad to broadcast in the newspaper that it cannot win if it cannot win\n",
            "the whole underpinning of the statute is sounder if you cannot win if you cannot win\n",
            "but you further have to have bases in the record which show that these practices have any bearing on validity of the statute\n",
            "speech cannot be regulated\n",
            "there is a sound foundation there and there has to be a sound one\n",
            "and if you cannot establish that, then there is no validity to the speech restriction\n",
            "does the have the same rule?\n",
            "so your argument is the same?\n",
            "it seems to me the same argument youve been making\n",
            "im not sure that has caught on in the courts\n",
            "im saying the winter issue in particular was very important to many\n",
            "my question is to people who are consumers of wine and its in their wine sales and their wine buying decisions\n",
            "when it comes to protected speech, the state has its own justification for discriminating\n",
            "it has its own justification\n",
            "well, do you think there a constitutional limit on the protected speech that can be achieved with this slight restriction?\n",
            "maybe maybe not\n",
            "do you have a specific record of sales and consumption patterns where the underorbersy has been fairly consistent?\n",
            "could you tell me, is the\n",
            "has the state done an adequate job of avoiding racial antagonism?\n",
            "in previous elections, did you see a significant increase in race-conscious sales tactics?\n",
            "in particular, a significant increase in the time that a sales rep usually gives to the issue after an election?\n",
            "was there a finding that there was a racial overinclusive effect?\n",
            "was there a finding that there was a bias occurring in the sales team?\n",
            "well, does the rationale or purport that you can use to find out the answer to that question lie in the record or is it simply the record, that the speech that the state is exhorting in the pre-election surveys that have been giving the most money to the super PACs that have given so much to the?\n",
            "is that what youre trying to show us, csis follow the money, the pre-election surveys?\n",
            "but it just the way it it is, it in the nature of favoritism\n",
            "if you find that it has the tendency to do favoritism, is that a fallacy?\n",
            "began our history by saying that we have to make a distinction between a national political party and a local political party because whenever you go out in the country you get a letter saying we have a memorandum of understanding, which means that you have to come to the home of the person who brings the package\n",
            "if the person is a member of the party headquarters, then that automatically brings us here as well\n",
            "what would you do in th attempt to establish the distinction, assuming the case we are on, assuming that there is just one cause of action?\n",
            "would you defend this on the grounds that there is no common law for a racial discrimination, or is that just an fallacy?\n",
            "it seems to me that you could do the third proposition\n",
            "that there is a common law for a racial discrimination, and that that discrimination is either authorized by law or permitted under law\n",
            "what are the common law, and have thought is an appropriate term\n",
            "is the the amount of products in a package that can be transported, and has found that there is a correlation between what the manufacturers do in the box and cross-products that consumers buy?\n",
            "what are the cross-products that consumers buy?\n",
            "well, does the answer to that question, whether or not there a common law for drug smuggling, as distinct from trafficking in a\n",
            "\n",
            "[810 | 1989.01] loss=0.12 avg=1.06\n",
            "[820 | 2012.94] loss=0.09 avg=1.04\n",
            "[830 | 2036.93] loss=0.11 avg=1.02\n",
            "[840 | 2060.88] loss=0.14 avg=1.01\n",
            "[850 | 2084.83] loss=0.10 avg=0.99\n",
            "[860 | 2108.79] loss=0.12 avg=0.98\n",
            "[870 | 2132.77] loss=0.15 avg=0.96\n",
            "[880 | 2156.78] loss=0.11 avg=0.95\n",
            "[890 | 2180.78] loss=0.12 avg=0.94\n",
            "[900 | 2204.78] loss=0.11 avg=0.92\n",
            "[910 | 2228.80] loss=0.10 avg=0.91\n",
            "[920 | 2252.80] loss=0.11 avg=0.89\n",
            "[930 | 2276.78] loss=0.10 avg=0.88\n",
            "[940 | 2300.79] loss=0.11 avg=0.87\n",
            "[950 | 2324.79] loss=0.12 avg=0.86\n",
            "[960 | 2348.82] loss=0.08 avg=0.84\n",
            "[970 | 2372.86] loss=0.11 avg=0.83\n",
            "[980 | 2396.91] loss=0.10 avg=0.82\n",
            "[990 | 2420.94] loss=0.09 avg=0.81\n",
            "[1000 | 2445.05] loss=0.07 avg=0.80\n",
            "Saving checkpoint/kennedy1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn2CJRzacBLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a28c7e6-7323-48c7-ad64-4aa3f7675c92"
      },
      "source": [
        "## from google.colab import drive\n",
        "## drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "In order to do this i just copied the whole thing over by draggging it. i guess for each justice i will just save their run accordingly.\n",
        "\n",
        "i downloaded the whole model directory (checkpoint output) locally so i can hopefully leverage https://github.com/nshepperd/gpt-2/blob/finetuning/src/interactive_conditional_samples.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## gpt2.copy_checkpoint_to_gdrive(run_name='kennedy1')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='kennedy1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='kennedy1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6a6fde6-aa7f-4914-8a91-9ecff771a892"
      },
      "source": [
        "gpt2.generate(sess, run_name='kennedy1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A patient and a sworn enemy,\n",
            "To one manly gentleman helps to make off\n",
            "One fairer feather: so, happily, I thank thee.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Welcome, Harry; welcome, Somerset: but that\n",
            "Me too shall be revenged on you.\n",
            "\n",
            "WARWICK:\n",
            "O happy friend I had, as it were a curse,\n",
            "To leave his country of so fair a life!\n",
            "\n",
            "SOMERSET:\n",
            "What foul beast is that in your chamber,\n",
            "Whose big ears and proud eyes shall slander thee\n",
            "For so blasphemous a charge?\n",
            "\n",
            "MONTAGUE:\n",
            "If I thought this was the sweetest, freshest, and lightest\n",
            "In all my cup of wine, I should be silent.\n",
            "It drinks me full of the sweets I drink,\n",
            "And makes me smile more than the sweetest flower.\n",
            "The more I drink, the more full I feel; and so\n",
            "The more sweet is the sweet drink I feel,\n",
            "And the more full full I feel.\n",
            "\n",
            "KING HENRY VI:\n",
            "Welcome to London, Somerset, and Lancaster.\n",
            "Welcome, cousin, to this comfortable bed;\n",
            "So naked and loathsome are these theots!\n",
            "\n",
            "QUEEN MARGARET:\n",
            "What, shall I waken a new man in my sight?\n",
            "\n",
            "KING HENRY VI:\n",
            "What, shall I waken a new man in your sight?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "What, shall I waken a new man in your sight?\n",
            "\n",
            "WARWICK:\n",
            "My countrymen are coming under heavy odds.\n",
            "\n",
            "MONTAGUE:\n",
            "The Earl of Wiltshire hath charged us to fight:\n",
            "If we refuse, he shall be Earl of March,\n",
            "And with him he shall lose his seat:\n",
            "If he take me he am going to be fought,\n",
            "Or he shall continue his seat as before.\n",
            "\n",
            "KING HENRY VI:\n",
            "My peaceable liege, abet thee:\n",
            "Abate the duke that hath charged me thee\n",
            "And send back, return, or I will taunt thee with death:\n",
            "The advantage I have in honour is in hate:\n",
            "Tuteling him for him, and that which is left him,\n",
            "Sith I mean my country in health and power,\n",
            "I will serve as soon as truth and right can.\n",
            "Return to page Montague, and tell them these terms:\n",
            "I will serve them as soon as truth and right can.\n",
            "Now, afore God, to thy heavy sorrows I say:\n",
            "I do reprehend thee in thy duty,\n",
            "In what to do for rejoicing, not in vengeance.\n",
            "\n",
            "BENVOLIO:\n",
            "Amen, combatants! for thy truly are set\n",
            "The bitter hours of goodby; and to thee we all bow\n",
            "Our monthly blessings.\n",
            "\n",
            "MONTAGUE:\n",
            "I hope the king shall not be so rough-faced.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "At what hour shall we puff our heavenly bodies?\n",
            "After our solemn solemnities have our powers:\n",
            "For our mutual solemnities we will sing the ebb and flow\n",
            "Of heavenly bodies; and our heavenly bodies,\n",
            "For our mutual heavenly bodies, we will sound\n",
            "The beat of our heavenly bodies; and our heavenly bodies,\n",
            "For our heavenly bodies, we will sing.\n",
            "Rest you, dear mourners; rest you, and rest;\n",
            "For, rest you, and rest for awhile, methinks,\n",
            "The like abstinence of your evils, methinks,\n",
            "You should with joy have heard, and you-or\n",
            "You, my dear ones, for a quieting rest.\n",
            "\n",
            "KING HENRY VI:\n",
            "You want a nap? you weep for joy;\n",
            "For you, my love, have been browbeat'd with millstones.\n",
            "\n",
            "MONTAGUE:\n",
            "And lack of any joy in absence?\n",
            "\n",
            "HENRY VI:\n",
            "(quietly sighs from head to toe))\n",
            "See who it is: here's a man of your blood,\n",
            "Whom God defend the present may have to bear.\n",
            "\n",
            "CLIFFORD:\n",
            "My lord, his grace, and the noble peers\n",
            "Have all urge'd against your highness.\n",
            "\n",
            "MONTAGUE:\n",
            "And what force dost thou in any respect oppose?\n",
            "\n",
            "YORK:\n",
            "That envies me, like a blasted farmyard mountain.\n",
            "\n",
            "CLIFFORD:\n",
            "Fear not my gloranimity; I'll be gone\n",
            "Even till I be proved aisance.\n",
            "\n",
            "MONTAGUE:\n",
            "And what harm doth he of thy life to thee?\n",
            "\n",
            "YORK:\n",
            "Ay, ay, Richard doth live, and doth entertain\n",
            "As figure to Richard live; whose life is forfeit\n",
            "If he be no better than a fool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "526dbd1b-f076-4f47-a42b-058ab4a35b9f"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LORD WILLOUGHBY:\n",
            "That, by the way, Clarence and I have done good side by side;\n",
            "And yet side we, and he side we have done ill.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why then 'tis done ill. O, how should I ease it?\n",
            "Side with him and my brother, my sovereign!\n",
            "Side wither away, and as night falls,\n",
            "Like to the farthest morning to my last,\n",
            "Side wither away, and as morning comes,\n",
            "Like to the furthest afternoon to my last!\n",
            "Side wither away, and as our fortunes turn,\n",
            "Like to the furthest afternoon to our last!\n",
            "\n",
            "QUEEN MARGARET:\n",
            "What is this? counsel? counsel!\n",
            "\n",
            "KING RICHARD II:\n",
            "My queen and my heir, for half a mile and a half\n",
            "She will glide this way, to be or no.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "So stands the orchard here, for half a mile and a!\n",
            "\n",
            "KING RICHARD II:\n",
            "So stands the orchard here, to fence it, to!\n",
            "Fashion it in her, like the hedgehog's net\n",
            "====================\n",
            "LORD STANLEY:\n",
            "What if I told you, in the hope of succor,\n",
            "That I had lain a little while in your arms?\n",
            "\n",
            "DUKE OF YORK:\n",
            "No doubt, my lord.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "'Tis a pity I should be coil'd to\n",
            "Be brief and unanswerable. Yet give me this.\n",
            "\n",
            "EXTON:\n",
            "'Tis a truth that vexes me deeply\n",
            "To try whether thou, Lord Hastings, art moved\n",
            "To enter publicly with gentle discourse\n",
            "And thanks from his acknowledged friends.\n",
            "\n",
            "HASTINGS:\n",
            "My gracious lord,\n",
            "Suppose me this: did I so love to see the Tower?\n",
            "\n",
            "KING RICHARD II:\n",
            "I did so; but the duelling Tower, moved\n",
            "By jealousies to oppress me,\n",
            "Wretches to usurp him held most dear,\n",
            "The truth is, I loved the Tower as I loved\n",
            "The princes that envied their prosperity.\n",
            "\n",
            "HASTINGS:\n",
            "I loved innocently,\n",
            "When my princes did usurp their gains; when\n",
            "When my grandsire and my liege, prince and prince,\n",
            "B\n",
            "====================\n",
            "LORD WILLOUGHBY:\n",
            "How long shall it take? only\n",
            "To behold your father's bending in the duke's.\n",
            "How long shall it be? do you understand me?\n",
            "\n",
            "ROMEO:\n",
            "Your grace, I understand you.\n",
            "\n",
            "ROMEO:\n",
            "It must be so; then I'll excuse myself.\n",
            "\n",
            "ROMEO:\n",
            "It shall be so.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Your nose is pleasant on myself.\n",
            "\n",
            "ROMEO:\n",
            "No need.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My heart is troubled by strange thoughts.\n",
            "\n",
            "ROMEO:\n",
            "No need.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My heart is troubled by strange thoughts.\n",
            "\n",
            "ROMEO:\n",
            "Prithee, be quiet; thou need'st it.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My mind is unsettled; no need.\n",
            "\n",
            "ROMEO:\n",
            "Prithee, be quiet; thou need'st it.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My mind is unsettled; no need.\n",
            "\n",
            "ROMEO:\n",
            "Prithee, be quiet; thou need'st it.\n",
            "\n",
            "FRIAR LA\n",
            "====================\n",
            "LORD WILLOUGHBY:\n",
            "And, if the right Edward were slain,\n",
            "My father's blood should wash the world from me.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "O, let him fly from me, that he may live!\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Arise, one last, and let him be slain ere he return.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "I thank God I am not young nor old to waste.\n",
            "I am young and wooer than this young wooer was.\n",
            "\n",
            "YORK:\n",
            "Younger than young, and wooer than a man is.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "What, wilt thou not kill her?\n",
            "\n",
            "RYBHUS:\n",
            "If thou darest, thou hast to do good deeds,\n",
            "If thou darest, thou darest not kill her, thou darest.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Where dost thou go? command Warwick to take her?\n",
            "\n",
            "WARWICK:\n",
            "Where leadess Warwick be that Warwick is woo'd.\n",
            "\n",
            "YORK:\n",
            "Where leadess Warwick be woo'd that noble York is.\n",
            "\n",
            "PRINCE\n",
            "====================\n",
            "LORD WILLOUGHBY:\n",
            "Let him please to come and sup with him?\n",
            "\n",
            "WARWICK:\n",
            "I promised he should come and sup with him.\n",
            "\n",
            "YORK:\n",
            "'Twas a vow of charity to vex him,\n",
            "And then he should vex us to come and sup.\n",
            "\n",
            "WARWICK:\n",
            "'Twas but a vow to come and sup with him.\n",
            "\n",
            "YORK:\n",
            "'Twas but a vow to come and sup with him.\n",
            "\n",
            "WARWICK:\n",
            "Come hither, slave boy.\n",
            "Me I come, you wretched hag, you wretched thing.\n",
            "\n",
            "DORCAS:\n",
            "'Tis very well. Come, go with me.\n",
            "\n",
            "WARWICK:\n",
            "I will be his slave, and make his bondage known.\n",
            "\n",
            "EXTON:\n",
            "So, you have resisted his bondage, you have run your errand too far.\n",
            "\n",
            "WESTMORELAND:\n",
            "O, but O, the slave that was past the year\n",
            "Doth not my errand a better errand?\n",
            "\n",
            "EXTON:\n",
            "No, for so my wits charge me thus too late.\n",
            "\n",
            "WARWICK:\n",
            "But\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2"
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}